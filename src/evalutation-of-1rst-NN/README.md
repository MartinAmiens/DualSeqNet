To assess the generation of an amino acid sequence, it was decided to produce a score based on the following criteria and observations:

# Is the protein functional?

1) Amino acid composition:

Natural distribution: The 20 amino acids are not equally represented in natural proteins. Some (such as Leucine, Alanine, Glycine, Serine) are very common, while others (such as Tryptophan, Cysteine, Histidine) are rarer. A random sequence will often have a distribution that is too uniform or too bizarre.

2) Presence of Key Amino Acids :

* Cysteine (C): Essential for the disulphide bridges that stabilise many 3D structures. A total absence or isolated presence would make the formation of these bridges impossible.

* Proline (P) and Glycine (G): Proline introduces rigid bends, while Glycine provides extreme flexibility. Their placement is crucial. A random sequence will place them anywhere, making folding difficult.

3) Hydrophobic/Hydrophilic balance:

Globular proteins (most functional proteins) generally have a hydrophobic core (amino acids that ‘don't like water’) and a hydrophilic surface (that ‘like water’). A random sequence will often have blocks of hydrophobic residues on the surface, or a chaotic mixture, favouring aggregation rather than correct folding.

4) Absence of Extreme and Simple Repeats :

Highly repetitive sequences such as AAAAA (poly-alanine) or PLPLPL are rare in functional globular proteins. They are more characteristic of regions of low complexity or fibrous structural proteins. A pure random sequence is likely to contain long repeats by pure chance, which is a sign of poor protein ‘design’ for a classical function.

5) Low Aggregation Potential:

Many random sequences contain “patches” of highly hydrophobic amino acids or amino acids with uncompensated charges which, when exposed to water, tend to clump together with other proteins, forming insoluble aggregates (like “lumps”). A “good” protein has evolved to avoid or mask these areas.

6) Propensity for Secondary Structure :

Amino acids have preferences for forming alpha helices, beta sheets or loops. A ‘good’ protein sequence has regions with a strong and consistent propensity to form these structures, which are the ‘building blocks’ of 3D folding. A random sequence will have a very heterogeneous and chaotic propensity, making the formation of a stable structure unlikely.

7) Absence of Internal Stop Codons (if from a complete translation) :

If you generate the sequence from DNA, it must start with a methionine (M) and must not contain any internal stop codons (TAA, TAG, TGA) that would stop translation prematurely.


# How to evaluate a protein generating model

When evaluating and scoring sequences generated by a protein model for your tool, it's crucial to adopt a robust and principled approach, as highlighted by the systematic analysis of metrics in the sources. The field of protein design currently lacks a single, standardized set of metrics, so many studies combine different approaches. Your evaluation should ideally encompass three key aspects: **quality, diversity, and distributional similarity**.

Here's a comprehensive guide to evaluating and scoring generated protein sequences:

### 1. Desirable Properties of Evaluation Metrics
Regardless of the specific metrics you choose, consider these general principles:
*   **Robustness and Sensitivity:** Metrics should be resilient to minor data perturbations but sensitive enough to detect meaningful differences in model performance.
*   **Interpretability:** They should provide clear insights into where your model can improve.
*   **Computational Efficiency:** Evaluation should be tractable, especially for monitoring during model training. The efficiency depends on the algorithm and its sensitivity to sample size.

### 2. Approaches to Evaluation Based on Data Availability
The choice of techniques often depends on whether you have access to the training data your model was based on:
*   **Only Generated Samples Available:** Focus on **quality** (how well samples meet desired characteristics) and **diversity** (variability within the generated set). These are often assessed using predefined functions or "oracle" models.
*   **Training Data Also Available:** You can extend your evaluation to include **fidelity** (how closely generated samples resemble real data) and **coverage** (how well generated samples span the full variability of the real data distribution).

### 3. Key Categories of Evaluation Metrics

#### A. Quality Metrics
These assess the characteristics of individual generated proteins, focusing on structural stability and sequence plausibility. A "good" protein should be structurally stable and self-consistent.
*   **Predicted Local Distance Difference Test (pLDDT):** A standard for evaluating structural quality, often predicted by AlphaFold, ESMFold, or OmegaFold. Higher pLDDT values generally indicate better structural quality, but it may underestimate quality in intrinsically disordered regions. ESMFold offers a good balance of accuracy and computational efficiency for pLDDT prediction.
*   **Perplexity (ppl):** Adapted from language modeling, it evaluates sequence quality. Lower perplexity indicates higher sequence plausibility. Autoregressive transformer protein language models like ProtGPT2, ProGen2, and RITA are used for its calculation. Perplexity shows remarkable consistency across various sample sizes.
*   **Pseudoperplexity (pppl):** An adaptation of perplexity for masked language models, often calculated using the ESM-2 family of models. While strongly correlated with perplexity (R2 > 0.94), it is computationally more demanding. Smaller ESM-2 models may suffice due to high correlations across different model complexities.
*   **Self-consistency perplexity (scPerplexity):** Leverages the sequence-structure relationship, defined as `-log p(S|G(F(S)))`, where F is a folding model and G is an inverse folding model. Lower scPerplexity suggests better alignment between the generated sequence and its predicted structure. It's comprehensive but computationally expensive and depends on the accuracy of the underlying folding and inverse folding models.
*   **Practical Recommendation for Quality:** A combination of **pLDDT and perplexity** is recommended to provide a balanced assessment, mitigating individual vulnerabilities (e.g., pLDDT with disordered regions, perplexity with low-complexity sequences). Alternatively, **scPerplexity** alone can be used if computational cost is not a barrier.

#### B. Diversity Metrics
These measure the variability within the set of generated proteins, ensuring the model isn't just memorizing training data.
*   **Cluster Density (CD):** Defined as the ratio of the number of clusters to the total number of sequences, using clustering techniques like MMseqs2 at different similarity thresholds (e.g., 50% for broader patterns, 95% for detecting mode collapse). CD calculations stabilize with **500-1000 samples**.
*   **Max ID (Maximum Identity):** Used in the ProGen study to measure the maximum identity of an artificial protein with any publicly available natural protein, allowing selection of sequences with controlled divergence from natural proteins.

#### C. Distributional Similarity Metrics
These compare generated and real protein distributions, providing insights into fidelity and diversity. They typically operate on distributions of protein vector representations (sequence-based or structure-based).
*   **Direct Measurement Approaches:**
    *   **Improved Precision and Recall (IPR):** Quantifies quality (precision: proportion of generated samples resembling real ones) and diversity (recall: proportion of real samples with generated analogues) in a high-dimensional feature space.
    *   **Density and Coverage (D&C):** Density measures generated samples within the real data manifold; coverage assesses real data manifold representation by generated samples.
    *   *Caution:* IPR and D&C metrics can exhibit erratic behavior and may have reduced sensitivity to subtle distributional shifts. Coverage and IPR Recall may show non-linear responses to imbalance and near-linear decline during elimination scenarios.
*   **Compound Metrics:** Implicitly account for both fidelity and diversity, and are more widely used.
    *   **Fréchet Distance (FD):** Quantifies dissimilarity between two multivariate Gaussian distributions. Fréchet Inception Distance (FID) is a variant used in image generation, applied to proteins using encoders like ProtT5 or ESM-1v.
    *   **Maximum Mean Discrepancy (MMD):** Measures the distance between two distributions in a reproducing kernel Hilbert space. The choice of the RBF kernel parameter σ is crucial; **σ = 10** is recommended for protein sequence evaluation as it balances sensitivity and consistent ordering. MMD can effectively measure diversity by analyzing changes in its components (e.g., RBF(q,q) for generated data self-similarity).
    *   **Earth Mover’s Distance (EMD):** Measures the minimum cost to transform one distribution into another, offering insights into diversity and proximity to the dataset.
*   **Practical Recommendation for Distributional Similarity:** **MMD (with RBF kernel σ = 10) and Fréchet Distance (FD)** are recommended for their balance of efficiency and reliability. These metrics are sensitive to overall distributional changes.
*   **Protein Representation Models:** The behavior of distributional distance metrics (FD, MMD, EMD) is consistent across different protein language models of varying sizes (e.g., ESM-2 8M to 3B, ProtT5), suggesting that the choice of embedding model is not critical for these metrics.

### 4. Sample Size Considerations
*   **General Rule:** All metrics tend to stabilize as sample size increases.
*   **Quality Metrics:** A minimum of **256 samples** is recommended, with **512 samples** offering more robust estimates, especially for scPerplexity or when analyzing sequences of unknown quality.
*   **Distributional Similarity Metrics:** At least **1024 samples** are advised for stable estimates.

### 5. Evaluating for Specific Goals/Failure Modes
*   **Novelty:** Assess the proportion of generated sequences not present in the training dataset. DiMA, for instance, focuses on producing genuinely novel proteins.
*   **Uniqueness:** Evaluate the model’s ability to generate diverse, non-repetitive outputs.
*   **Homology:** HMMER scores can assess how well generated sequences are homologs of the protein family.
*   **Coevolution:** Statistical energy scores derived from Potts models (e.g., bmDCA) or protein language models like MSA Transformer can account for coevolution.
*   **Structural Similarity:** RMSD (root-mean-square deviation) between predicted structures and experimental ones can assess similarity. AlphaFold's pLDDT scores can indicate confidence in predicted structure.
*   **Functional Relevance:** Beyond statistical measures, experimental validation is key to confirm enhanced properties or functionality. For example, SAGE-Prot uses QSPR models to predict properties like binding affinity, thermal stability, enzymatic activity, and solubility. ProGen uses model log-likelihoods and adversarial discriminator scores to predict functionality.
*   **Detecting Mode Collapse:** Cluster Density at a 95% similarity threshold can identify scenarios where the model generates nearly identical sequences.
*   **Detecting Overrepresentation of Families:** MMD is sensitive to controlled perturbations that simulate overrepresentation.

### 6. Computational Efficiency for Workflow Stages
*   **Rapid Evaluation During Model Development:** Prioritize **perplexity for quality** and **MMD (with optimized σ)** for distributional similarity due to their efficiency.
*   **Comprehensive Final Evaluation:** Use a combination of **pLDDT with perplexity (or scPerplexity), Cluster Density, and MMD or Fréchet Distance**. Autoregressive models like arDCA are also noted for their computational efficiency in generating sequences and predicting mutational effects, performing comparably to more complex models at a significantly lower cost.

In essence, evaluating generated protein sequences is like assessing the craftsmanship of a new type of key. You need to check if each key (sequence) is well-formed (quality), if you have a wide variety of keys (diversity), and if your new set of keys accurately reflects the range of all existing valid keys (distributional similarity). Just as a locksmith would use different tools and tests to verify a key's function, from visual inspection to trying it in various locks, you'll need a suite of metrics and potentially experimental validation to truly understand the utility and reliability of your generated protein sequences.